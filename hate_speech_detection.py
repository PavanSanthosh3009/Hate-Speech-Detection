# -*- coding: utf-8 -*-
"""Hate Speech Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZRCjhCZWJeZIe3Nrc08C7Q82G5gzxwZg
"""

import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# **Import library**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
# %matplotlib inline
from sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score
from sklearn.multiclass import OneVsRestClassifier
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

"""# **Read dataset**"""

from google.colab import drive
drive.mount('/content/drive')

import csv
traindf = pd.read_csv("/content/drive/MyDrive/train.csv")
testdf = pd.read_csv("/content/drive/MyDrive/test.csv")

traindf.head(10)

traindf[traindf['toxic']==1].head(2)

"""# **Data Preprocessing**"""

cols_target = []
i = 0
for col in traindf.columns:
    if i == 0 or i == 1:
        i += 1
        continue
    cols_target.append(col)

cols_target

# check missing values in numeric columns
traindf.describe()

len(traindf['toxic'].drop_duplicates())

traindf[traindf['comment_text'].isnull()]

testdf[testdf['comment_text'].isnull()]

print('Total rows in test is {}'.format(len(testdf)))
print('Total rows in train is {}'.format(len(traindf)))
print(traindf[cols_target].sum())

data = traindf[cols_target]

colormap = plt.cm.plasma
plt.figure(figsize=(7,7))
plt.title('Correlation of features & targets',y=1.05,size=14)
sns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,
           linecolor='white',annot=True)

"""# **Function to Clean the data**"""

def cleanData(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = text.strip(' ')
    return text

traindf['comment_text'] = traindf['comment_text'].apply(lambda x:cleanData(x))
testdf['comment_text'] = testdf['comment_text'].apply(lambda x:cleanData(x))

"""# **Removing of stopWords**"""

traindf['comment_text'].head(5)

import nltk

nltk.download('stopwords')
nltk.download('wordnet')

eng_stopwords = set(stopwords.words("english"))

traindf['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))

traindf['comment_text'] = traindf['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))
testdf['comment_text'] = testdf['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))

traindf['comment_text'].head(5)

"""# **Applying Lemmatization or Stemming**"""

stemmer = PorterStemmer()
wordnet_lemmatizer = WordNetLemmatizer()

# stemming and lemmatizing
# adapted from the kernal

def stem_word(text):
    txt = " ".join([stemmer.stem(w) for w in text.split()])
    return text

def lemmatize_word(text):
    txt = " ".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])
    return txt

sample = "hello how are you doing"
lemmatize_word(sample)

SL = -1

traindf

toxic_temp_df = traindf[traindf['toxic']==1]

toxic_temp_df

if(SL == 1):
    traindf['comment_text'] = traindf['comment_text'].map(lambda x: stem_word(x))
    testdf['comment_text'] = testdf['comment_text'].map(lambda x: stem_word(x))
else:
    traindf['comment_text'] = traindf['comment_text'].map(lambda x: lemmatize_word(x))
    testdf['comment_text'] = testdf['comment_text'].map(lambda x: lemmatize_word(x))

traindf.head()

traindf['comment_text']

X = traindf.comment_text
Y = traindf.drop(['id', 'comment_text'], axis = 1)

print(X.shape, Y.shape)

X_train,X_test, y_train,y_test= train_test_split(X,Y,test_size=0.2, random_state=42)

X_train.head(2)

y_train.head(2)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""# **Term Frequency Inverse Document Frequency Vectorizer**"""

X_train

word_vectorizer = TfidfVectorizer(
    strip_accents='unicode',
    analyzer='word',
    token_pattern=r'\w{1,}',
    ngram_range=(1, 3),
    sublinear_tf=True)

word_vectorizer.fit(X_train)
train_word_features = word_vectorizer.transform(X_train)
test_features = word_vectorizer.transform(X_test)

print(test_features)

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

"""# **Model Validation on train data set**"""

losses = []
auc = []

for class_name in cols_target:
    #call the labels one column at a time so we can run the classifier on them
    train_target = y_train[class_name]
    test_target = y_test[class_name]
    classifier = LogisticRegression(solver='sag', C=10)

    cv_loss = np.mean(cross_val_score(classifier, train_word_features, train_target, cv=5, scoring='neg_log_loss'))
    losses.append(cv_loss)
    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))

    cv_score = np.mean(cross_val_score(classifier, train_word_features, train_target, cv=5, scoring='accuracy'))
    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))

    classifier.fit(train_word_features, train_target)
    y_pred = classifier.predict(test_features)
    y_pred_prob = classifier.predict_proba(test_features)[:, 1]
    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)
    auc.append(auc_score)
    print("CV ROC_AUC score {}\n".format(auc_score))

    print(confusion_matrix(test_target, y_pred))
    print(classification_report(test_target, y_pred))

print('Total average CV Log_loss score is {}'.format(np.mean(losses)))
print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))

"""# **Test Prediction**"""

def make_test_predictions(df,classifier):
    df.comment_text = df.comment_text.apply(cleanData)
    X_test = df.comment_text
    X_test_transformed = word_vectorizer.transform(X_test)
    y_test_pred = classifier.predict_proba(X_test_transformed)
    result =  sum(y_test_pred[0])
    if result >=1 :
       return("Toxic Comment")
    else :
       return ("NonToxic Comment")

log_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=45)
classifier = OneVsRestClassifier(log_reg)
classifier.fit(train_word_features, y_train)



comment_text = "I hate you idiot , get lost"
comment ={'id':[565],'comment_text':[comment_text]}
comment = pd.DataFrame(comment)
result = make_test_predictions(comment,classifier)
print(result)

comment_text = "Sorry i dont know you"
comment ={'id':[565],'comment_text':[comment_text]}
comment = pd.DataFrame(comment)
result = make_test_predictions(comment,classifier)
print(result)

comment_text = "May I know who are you"
comment ={'id':[565],'comment_text':[comment_text]}
comment = pd.DataFrame(comment)
result = make_test_predictions(comment,classifier)
print(result)

